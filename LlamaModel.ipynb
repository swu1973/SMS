{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LlamaModel Code\n",
        "\n",
        "The LlamaModel is a continuation of Mallory Helfenbein's (NASA HQ intern 2023) [ReviewerExtractor codeV2](https://github.com/ninoc/ReviewerExtractor/tree/main/codeV2). You must create an ADS account and obtain an [api token](https://ui.adsabs.harvard.edu/user/settings/token). We input a list of researcher names into the codeV2 which searches by first author in ADS, gather their abstracts from 2003 to 2030, and returns the top 10 words, bigrams, and trigrams. From these n-grams, we create a combined top words list.\n",
        "\n",
        "We used the llama3-70b-8192 model in groqCloud. You must create a groqCloud account and obtain an [api key](https://console.groq.com/keys). The llama model takes in the combined top words for each researcher and will determine the expertise chosen from the [AAS keywords](https://journals.aas.org/keywords-2013/). We fed the model a specific prompt and the specific topics from AAS. First, the model is prompted to determine the general topics and then it is asked for their associated subtopics.\n",
        "\n",
        "###Citing this code:\n",
        "\n",
        "Part of this code is the second version of a Expertise finding tool developed by Helfenbein et al. 2023.\n",
        "\n",
        "It utilizes NASA ADS API to query for articles (refereed or not) in the \"Astronomy\" database (cite ADS). Please, cite \"Wu & Lendahl et al. 2024\" and refer to the README file in the github.\n"
      ],
      "metadata": {
        "id": "JciXopF9TsmJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Packages\n",
        "\n",
        "*   pandas 1.5.3\n",
        "*   groqCloud api\n",
        "*   mount to google drive\n",
        "*   nltk for n-grams\n",
        "*   ads api\n",
        "*   TextAnalysis.py\n",
        "*   stopwords.txt (to create meaningful N-grams)\n",
        "*   ADSsearcherpkg.py"
      ],
      "metadata": {
        "id": "8HURbDDBYfYI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPTPPbuFsppv"
      },
      "outputs": [],
      "source": [
        "#We designed the code to work with Pandas 1.5.3. If the session restarts, make sure to run this cell again to import the correct version.\n",
        "import pandas as pd\n",
        "print(pd. __version__)\n",
        "\n",
        "#If the Pandas version differs from 1.5.3, run the following:\n",
        "!pip install pandas==1.5.3 --user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5cMidRJsb7z"
      },
      "outputs": [],
      "source": [
        "#connect to your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJqOhlhYsnvq"
      },
      "outputs": [],
      "source": [
        "#imports for n-grams\n",
        "import requests\n",
        "from urllib.parse import urlencode, quote_plus\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQI8RBD4sohJ"
      },
      "outputs": [],
      "source": [
        "#install ads package\n",
        "!pip install ads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7CAl0ihsrTU"
      },
      "outputs": [],
      "source": [
        "#create a folder in your drive. In this case we named it SMS. You must store the TextAnalysis.py, stopwords.txt, ADSsearcherpkg.py, and the SMS Data in this folder\n",
        "path_stop= '/content/drive/MyDrive/SMS/'\n",
        "stop_file='stopwords.txt'\n",
        "stop_dir=path_stop+stop_file\n",
        "sys.path.append(path_stop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VB7azsTSstoX"
      },
      "outputs": [],
      "source": [
        "#For the TextAnalysis File, please refer to M. Volze et al. 2023\n",
        "import TextAnalysis as TA\n",
        "import ADSsearcherpkg as AP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPUCX8iPO2Vi"
      },
      "outputs": [],
      "source": [
        "#Grab the data. It is best to run a large amount of data in chunks. Here we did chunks of a 1000.\n",
        "SMS_file = pd.read_csv('/content/drive/MyDrive/SMS/SMS Input.csv') #import the total SMS data\n",
        "first_1000 = SMS_file[0:1000] #change range to get a subset of the data (ex: the first 1000 rows you want to run through the llama model)\n",
        "\n",
        "file_name = '/content/drive/MyDrive/SMS/0-1000.csv'\n",
        "first_1000.to_csv(file_name, index=False) #save this file to a csv in your folder\n",
        "first_1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3TINMzlsvYV"
      },
      "outputs": [],
      "source": [
        "token = '' #Insert your ADS API token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tcQNTNMppqF"
      },
      "source": [
        "## Reviewer Extractor\n",
        "\n",
        "Import a file of researcher names as a csv file (a column labeled \"Name\" and formatted Last Name, First Name) and run the names through ADS Search. This will search by first author, gather their abstracts from 2003 to 2030, and return the top n-grams."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiQeWmYhVy6D"
      },
      "outputs": [],
      "source": [
        "#save the SMS chunk of data aside\n",
        "file_name = '/content/drive/MyDrive/SMS/0-1000.csv'\n",
        "sample_df = pd.read_csv(file_name)\n",
        "sample_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFIIlbqCtDwU"
      },
      "outputs": [],
      "source": [
        "#ADS Search from codeV2\n",
        "datf=AP.run_file_names(filename=file_name,\n",
        "               token=token, stop_dir=stop_dir)\n",
        "\n",
        "datf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nElKa9h-X47I"
      },
      "outputs": [],
      "source": [
        "#Save the people with no ADS search results to a separate excel file\n",
        "ads_no_results = sample_df[sample_df[\"Name\"].isin(datf[\"Input Author\"]) == False]\n",
        "ads_no_results.to_excel(path_stop+'ADS_no_results1.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "py3N2nxGp_Wg"
      },
      "outputs": [],
      "source": [
        "#Combined top words function\n",
        "import itertools\n",
        "def topwords(top10words, top10bigrams, top10trigrams):\n",
        "    '''\n",
        "    Takes in a list of top 10 words, bigrams, and trigrams and returns a combined list\n",
        "    '''\n",
        "    # Handle the case where input might not be lists of tuples\n",
        "    if isinstance(top10words, str):\n",
        "        top10words = eval(top10words) # Safely evaluate string representation of list\n",
        "    if isinstance(top10bigrams, str):\n",
        "        top10bigrams = eval(top10bigrams)\n",
        "    if isinstance(top10trigrams, str):\n",
        "        top10trigrams = eval(top10trigrams)\n",
        "\n",
        "    topwords = [word for word, _ in top10words]\n",
        "    topbigrams = [' '.join(words) for words, _ in top10bigrams]\n",
        "    toptrigrams = [' '.join(words) for words, _ in top10trigrams]\n",
        "\n",
        "    lst = [topwords, topbigrams, toptrigrams]\n",
        "    single_list = list(itertools.chain(*lst))\n",
        "    return single_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knodfSZtqxit"
      },
      "source": [
        "Combine the N-grams into a single list using the function above. Also append the number of research papers they wrote as first author from 2003 to 2030."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdjSCRqnGakL"
      },
      "outputs": [],
      "source": [
        "#Gather the top combined words and number of papers\n",
        "datf[\"Top Combined Words\"] = datf.apply(lambda x: topwords(x[\"Top 10 Words\"], x[\"Top 10 Bigrams\"], x['Top 10 Trigrams']), axis=1)\n",
        "datf[\"Number of Papers\"] = datf[\"Bibcode\"].str.split(\",\").str.len()\n",
        "datf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-SXOo5tERhL"
      },
      "outputs": [],
      "source": [
        "#drop duplicates and reset the index\n",
        "datf = datf.astype(str) #convert datf columns to type string so you can drop duplicates\n",
        "datf.drop_duplicates(inplace=True)\n",
        "datf.reset_index(drop=True, inplace=True)\n",
        "datf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BgBjs1TRRSa"
      },
      "outputs": [],
      "source": [
        "#Save the ADS search results file. This is the file you'll need to run through the llama model\n",
        "datf.to_excel(path_stop+'ADS_results1.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (START HERE IF CODE ERRORS AFTER RUNNING THE LLAMA MODEL) Re-Import ADS Search Results"
      ],
      "metadata": {
        "id": "GcDbnMpcYyyd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4lfIggX89T7"
      },
      "outputs": [],
      "source": [
        "#Skip this if this is the first time you are running the code\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "path_stop= '/content/drive/MyDrive/SMS/'\n",
        "output_file = path_stop+\"output_0-1000.csv\" #make sure you have the right output_file name\n",
        "sample_df = pd.read_csv('/content/drive/MyDrive/SMS/0-1000.csv') #make sure file name is correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba1vHL5ugcxn"
      },
      "outputs": [],
      "source": [
        "#Re-import ADS Search results file here if the code errors. Skip this if this is the first time you are running the code.\n",
        "df = pd.read_excel(path_stop+'ADS_results1.xlsx')\n",
        "datf = df[178:] #slice where the code stopped\n",
        "datf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vivHlwGGrKXC"
      },
      "source": [
        "## Preparing the Llama Model\n",
        "\n",
        "*   Run the dictionary of topics and their subtopics\n",
        "*   Run the topic and subtopic prompts for the model\n",
        "*   extract_topics function to extract the expertise from the model's response\n",
        "*   generate_subtopics function to generate the subtopics with the model\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TD6ctUIEqTuJ"
      },
      "outputs": [],
      "source": [
        "# @title Run the Subtopics Dictionary\n",
        "d = {\n",
        "\"physical data and processes\":\n",
        "\"\"\"\n",
        "acceleration of particles\n",
        "accretion, accretion disks\n",
        "asteroseismology\n",
        "astrobiology\n",
        "astrochemistry\n",
        "astroparticle physics\n",
        "atomic data\n",
        "atomic processes\n",
        "black hole physics\n",
        "chaos\n",
        "conduction\n",
        "convection\n",
        "dense matter\n",
        "diffusion\n",
        "dynamo\n",
        "elementary particles\n",
        "equation of state\n",
        "gravitation\n",
        "gravitational lensing: strong\n",
        "gravitational lensing: weak\n",
        "gravitational lensing: micro\n",
        "gravitational waves\n",
        "hydrodynamics\n",
        "instabilities\n",
        "line: formation\n",
        "line: identification\n",
        "line: profiles\n",
        "magnetic fields\n",
        "magnetic reconnection\n",
        "magnetohydrodynamics (MHD)\n",
        "masers\n",
        "molecular data\n",
        "molecular processes\n",
        "neutrinos\n",
        "nuclear reactions, nucleosynthesis, abundances\n",
        "opacity\n",
        "plasmas\n",
        "polarization\n",
        "radiation: dynamics\n",
        "radiation mechanisms: general\n",
        "radiation mechanisms: non-thermal\n",
        "radiation mechanisms: thermal\n",
        "radiative transfer\n",
        "relativistic processes\n",
        "scattering\n",
        "shock waves\n",
        "solid state: refractory\n",
        "solid state: volatile\n",
        "turbulence\n",
        "waves\n",
        "\"\"\",\n",
        "\"astronomical instrumentation methods and techniques\":\n",
        "\"\"\"\n",
        "atmospheric effects\n",
        "balloons\n",
        "instrumentation: adaptive optics\n",
        "instrumentation: detectors\n",
        "instrumentation: high angular resolution\n",
        "instrumentation: interferometers\n",
        "instrumentation: miscellaneous\n",
        "instrumentation: photometers\n",
        "instrumentation: polarimeters\n",
        "instrumentation: spectrographs\n",
        "light pollution\n",
        "methods: analytical\n",
        "methods: data analysis\n",
        "methods: laboratory: atomic\n",
        "methods: laboratory: molecular\n",
        "methods: laboratory: solid state\n",
        "methods: miscellaneous\n",
        "methods: numerical\n",
        "methods: observational\n",
        "methods: statistical\n",
        "site testing\n",
        "space vehicles\n",
        "space vehicles: instruments\n",
        "techniques: high angular resolution\n",
        "techniques: image processing\n",
        "techniques: imaging spectroscopy\n",
        "techniques: interferometric\n",
        "techniques: miscellaneous\n",
        "techniques: photometric\n",
        "techniques: polarimetric\n",
        "techniques: radar astronomy\n",
        "techniques: radial velocities\n",
        "techniques: spectroscopic\n",
        "telescopes\n",
        "\"\"\",\n",
        "\"astronomical databases\":\n",
        "\"\"\"\n",
        "astronomical databases: miscellaneous\n",
        "atlases\n",
        "catalogs\n",
        "surveys\n",
        "virtual observatory tools\n",
        "\"\"\",\n",
        "\"astrometry and celestial mechanics\":\n",
        "\"\"\"\n",
        "astrometry\n",
        "celestial mechanics\n",
        "eclipses\n",
        "ephemerides\n",
        "occultations\n",
        "parallaxes\n",
        "proper motions\n",
        "reference systems\n",
        "time\n",
        "\"\"\",\n",
        "\"the sun\":\n",
        "\"\"\"\n",
        "Sun: abundances\n",
        "Sun: activity\n",
        "Sun: atmosphere\n",
        "Sun: chromosphere\n",
        "Sun: corona\n",
        "Sun: coronal mass ejections (CMEs)\n",
        "Sun: evolution\n",
        "Sun: faculae, plages\n",
        "Sun: filaments, prominences\n",
        "Sun: flares\n",
        "Sun: fundamental parameters\n",
        "Sun: general\n",
        "Sun: granulation\n",
        "Sun: helioseismology\n",
        "Sun: heliosphere\n",
        "Sun: infrared\n",
        "Sun: interior\n",
        "Sun: magnetic fields\n",
        "Sun: oscillations\n",
        "Sun: particle emission\n",
        "Sun: photosphere\n",
        "Sun: radio radiation\n",
        "Sun: rotation\n",
        "(Sun:) solar–terrestrial relations\n",
        "(Sun:) solar wind\n",
        "(Sun:) sunspots\n",
        "Sun: transition region\n",
        "Sun: UV radiation\n",
        "Sun: X-rays, gamma rays\n",
        "\"\"\",\n",
        "\"planetary systems\":\n",
        "\"\"\"\n",
        "comets: general\n",
        "comets: individual (…, …)\n",
        "Earth\n",
        "interplanetary medium\n",
        "Kuiper belt: general\n",
        "Kuiper belt objects: individual (…, …)\n",
        "meteorites, meteors, meteoroids\n",
        "minor planets, asteroids: general\n",
        "minor planets, asteroids: individual (…, …)\n",
        "Moon\n",
        "Oort Cloud\n",
        "planets and satellites: atmospheres\n",
        "planets and satellites: aurorae\n",
        "planets and satellites: composition\n",
        "planets and satellites: detection\n",
        "planets and satellites: dynamical evolution and stability\n",
        "planets and satellites: formation\n",
        "planets and satellites: fundamental parameters\n",
        "planets and satellites: gaseous planets\n",
        "planets and satellites: general\n",
        "planets and satellites: individual (…, …)\n",
        "planets and satellites: interiors\n",
        "planets and satellites: magnetic fields\n",
        "planets and satellites: oceans\n",
        "planets and satellites: physical evolution\n",
        "planets and satellites: rings\n",
        "planets and satellites: surfaces\n",
        "planets and satellites: tectonics\n",
        "planets and satellites: terrestrial planets\n",
        "protoplanetary disks\n",
        "planet–disk interactions\n",
        "planet–star interactions\n",
        "zodiacal dust\n",
        "\"\"\",\n",
        "\"stars\":\n",
        "\"\"\"\n",
        "stars: abundances\n",
        "stars: activity\n",
        "stars: AGB and post-AGB\n",
        "stars: atmospheres\n",
        "(stars:) binaries (including multiple): close\n",
        "(stars:) binaries: eclipsing\n",
        "(stars:) binaries: general\n",
        "(stars:) binaries: spectroscopic\n",
        "(stars:) binaries: symbiotic\n",
        "(stars:) binaries: visual\n",
        "stars: black holes\n",
        "(stars:) blue stragglers\n",
        "(stars:) brown dwarfs\n",
        "stars: carbon\n",
        "stars: chemically peculiar\n",
        "stars: chromospheres\n",
        "(stars:) circumstellar matter\n",
        "stars: coronae\n",
        "stars: distances\n",
        "stars: dwarf novae\n",
        "stars: early-type\n",
        "stars: emission-line, Be\n",
        "stars: evolution\n",
        "stars: flare\n",
        "stars: formation\n",
        "stars: fundamental parameters\n",
        "stars: general\n",
        "(stars:) gamma-ray burst: general\n",
        "(stars:) gamma-ray burst: individual (…, …)\n",
        "(stars:) Hertzsprung–Russell and C–M diagrams\n",
        "stars: horizontal-branch\n",
        "stars: imaging\n",
        "stars: individual (…, …)\n",
        "stars: interiors\n",
        "stars: jets\n",
        "stars: kinematics and dynamics\n",
        "stars: late-type\n",
        "stars: low-mass\n",
        "stars: luminosity function, mass function\n",
        "stars: magnetars\n",
        "stars: magnetic field\n",
        "stars: massive\n",
        "stars: mass-loss\n",
        "stars: neutron\n",
        "(stars:) novae, cataclysmic variables\n",
        "stars: oscillations (including pulsations)\n",
        "stars: peculiar (except chemically peculiar)\n",
        "(stars:) planetary systems\n",
        "stars: Population II\n",
        "stars: Population III\n",
        "stars: pre-main sequence\n",
        "stars: protostars\n",
        "(stars:) pulsars: general\n",
        "(stars:) pulsars: individual (…, …)\n",
        "stars: rotation\n",
        "stars: solar-type\n",
        "(stars:) starspots\n",
        "stars: statistics\n",
        "(stars:) subdwarfs\n",
        "(stars:) supergiants\n",
        "(stars:) supernovae: general\n",
        "(stars:) supernovae: individual (…, …)\n",
        "stars: variables: Cepheids\n",
        "stars: variables: delta Scuti\n",
        "stars: variables: general\n",
        "stars: variables: RR Lyrae\n",
        "stars: variables: S Doradus\n",
        "stars: variables: T Tauri, Herbig Ae/Be\n",
        "(stars:) white dwarfs\n",
        "stars: winds, outflows\n",
        "stars: Wolf–Rayet\n",
        "\"\"\",\n",
        "\"interstellar medium (ism) nebulae\":\n",
        "\"\"\"\n",
        "ISM: abundances\n",
        "ISM: atoms\n",
        "ISM: bubbles\n",
        "ISM: clouds\n",
        "(ISM:) cosmic rays\n",
        "(ISM:) dust, extinction\n",
        "(ISM:) evolution\n",
        "ISM: general\n",
        "(ISM:) HII regions\n",
        "(ISM:) Herbig–Haro objects\n",
        "ISM: individual objects (…, …) (except\n",
        "planetary nebulae)\n",
        "ISM: jets and outflows\n",
        "ISM: kinematics and dynamics\n",
        "ISM: lines and bands\n",
        "ISM: magnetic fields\n",
        "ISM: molecules\n",
        "(ISM:) planetary nebulae: general\n",
        "(ISM:) planetary nebulae: individual (…, …)\n",
        "(ISM:) photon-dominated region (PDR)\n",
        "ISM: structure\n",
        "ISM: supernova remnants\n",
        "\"\"\",\n",
        "\"the galaxy\":\n",
        "\"\"\"\n",
        "Galaxy: abundances\n",
        "Galaxy: bulge\n",
        "Galaxy: center\n",
        "Galaxy: disk\n",
        "Galaxy: evolution\n",
        "Galaxy: formation\n",
        "Galaxy: fundamental parameters\n",
        "Galaxy: general\n",
        "(Galaxy:) globular clusters: general\n",
        "(Galaxy:) globular clusters: individual (…, …)\n",
        "Galaxy: halo\n",
        "(Galaxy:) local interstellar matter\n",
        "Galaxy: kinematics and dynamics\n",
        "Galaxy: nucleus\n",
        "(Galaxy:) open clusters and associations: general\n",
        "(Galaxy:) open clusters and associations: individual (…, …)\n",
        "(Galaxy:) solar neighborhood\n",
        "Galaxy: stellar content\n",
        "Galaxy: structure\n",
        "\"\"\",\n",
        "\"galaxies\":\n",
        "\"\"\"\n",
        "galaxies: abundances\n",
        "galaxies: active\n",
        "(galaxies:) BL Lacertae objects: general\n",
        "(galaxies:) BL Lacertae objects: individual (…, …)\n",
        "galaxies: bulges\n",
        "galaxies: clusters: general\n",
        "galaxies: clusters: individual (…, …)\n",
        "galaxies: clusters: intracluster medium\n",
        "galaxies: distances and redshifts\n",
        "galaxies: dwarf\n",
        "galaxies: elliptical and lenticular, cD\n",
        "galaxies: evolution\n",
        "galaxies: formation\n",
        "galaxies: fundamental parameters\n",
        "galaxies: general\n",
        "galaxies: groups: general\n",
        "galaxies: groups: individual (…, …)\n",
        "galaxies: halos\n",
        "galaxies: high-redshift\n",
        "galaxies: individual (…, …)\n",
        "galaxies: interactions\n",
        "(galaxies:) intergalactic medium\n",
        "galaxies: irregular\n",
        "galaxies: ISM\n",
        "galaxies: jets\n",
        "galaxies: kinematics and dynamics\n",
        "(galaxies:) Local Group\n",
        "galaxies: luminosity function, mass function\n",
        "(galaxies:) Magellanic Clouds\n",
        "galaxies: magnetic fields\n",
        "galaxies: nuclei\n",
        "galaxies: peculiar\n",
        "galaxies: photometry\n",
        "(galaxies:) quasars: absorption lines\n",
        "(galaxies:) quasars: emission lines\n",
        "(galaxies:) quasars: general\n",
        "(galaxies:) quasars: individual (…, …)\n",
        "(galaxies:) quasars: supermassive black holes\n",
        "galaxies: Seyfert\n",
        "galaxies: spiral\n",
        "galaxies: starburst\n",
        "galaxies: star clusters: general\n",
        "galaxies: star clusters: individual (…, …)\n",
        "galaxies: star formation\n",
        "galaxies: statistics\n",
        "galaxies: stellar content\n",
        "galaxies: structure\n",
        "\"\"\",\n",
        "\"cosmology\":\n",
        "\"\"\"\n",
        "(cosmology:) cosmic background radiation\n",
        "(cosmology:) cosmological parameters\n",
        "cosmology: miscellaneous\n",
        "cosmology: observations\n",
        "cosmology: theory\n",
        "(cosmology:) dark ages, reionization, first stars\n",
        "(cosmology:) dark matter\n",
        "(cosmology:) dark energy\n",
        "(cosmology:) diffuse radiation\n",
        "(cosmology:) distance scale\n",
        "(cosmology:) early universe\n",
        "(cosmology:) inflation\n",
        "(cosmology:) large-scale structure of universe\n",
        "(cosmology:) primordial nucleosynthesis\n",
        "\"\"\",\n",
        "\"resolved and unresolved sources as a function of wavelength\":\n",
        "\"\"\"\n",
        "gamma rays: diffuse background\n",
        "gamma rays: galaxies\n",
        "gamma rays: galaxies: clusters\n",
        "gamma rays: general\n",
        "gamma rays: ISM\n",
        "gamma rays: stars\n",
        "infrared: diffuse background\n",
        "infrared: galaxies\n",
        "infrared: general\n",
        "infrared: ISM\n",
        "infrared: planetary systems\n",
        "infrared: stars\n",
        "radio continuum: galaxies\n",
        "radio continuum: general\n",
        "radio continuum: ISM\n",
        "radio continuum: planetary systems\n",
        "radio continuum: stars\n",
        "radio lines: galaxies\n",
        "radio lines: general\n",
        "radio lines: ISM\n",
        "radio lines: planetary systems\n",
        "radio lines: stars\n",
        "submillimeter: diffuse background\n",
        "submillimeter: galaxies\n",
        "submillimeter: general\n",
        "submillimeter: ISM\n",
        "submillimeter: planetary systems\n",
        "submillimeter: stars\n",
        "ultraviolet: galaxies\n",
        "ultraviolet: general\n",
        "ultraviolet: ISM\n",
        "ultraviolet: planetary systems\n",
        "ultraviolet: stars\n",
        "X-rays: binaries\n",
        "X-rays: bursts\n",
        "X-rays: diffuse background\n",
        "X-rays: galaxies\n",
        "X-rays: galaxies: clusters\n",
        "X-rays: general\n",
        "X-rays: individual (…, …)\n",
        "X-rays: ISM\n",
        "X-rays: stars\n",
        "\"\"\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4TjJ4Vze5bF"
      },
      "outputs": [],
      "source": [
        "#Llama model prompts\n",
        "prompt_topic = \"\"\"\n",
        "You are a scientist determining the areas of expertise of a person based on a list of top words from their abstracts pulled from the NASA Astrophysics Data System (ADS). You will be provided a list of top words to determine the topic or topics the person is an expert in. The topics are listed below.\n",
        "\n",
        "Topics list:\n",
        "physical data and processes\n",
        "astronomical instrumentation methods and techniques\n",
        "astronomical databases\n",
        "astrometry and celestial mechanics\n",
        "the sun\n",
        "planetary systems\n",
        "stars\n",
        "interstellar medium (ism) nebulae\n",
        "the galaxy\n",
        "galaxies\n",
        "cosmology\n",
        "resolved and unresolved sources as a function of wavelength\n",
        "\n",
        "Now please determine accurately the topic or topics based on these top words and provide evidence. You MUST ONLY choose from the topics list above. Choose to the best of your ability and do not leave the response as none. Please list out the topics in a python list format first. Here are some examples of the expected format, [galaxies, the sun, cosmology], [astronomical databases], [interstellar medium (ism) nebulae, astronomical instrumentation methods and techniques, resolved and unresolved sources as a function of wavelength, stars]. Note these examples contain topics chosen ONLY from the topics list and do not include the ` character. Then provide evidence by explaining which of the top words correspond with the topic. Here are the top words.:\n",
        "\"\"\"\n",
        "\n",
        "def prompt_subtopic(subtopic):\n",
        "  prompt = \"\"\"\n",
        "  You are a scientist determining the specific areas of expertise of a person based on a list of top words from their abstracts pulled from the NASA Astrophysics Data System (ADS) and their associated general topic. You will be provided a list of top words and the general topic to determine the subtopic or subtopics the person is an expert in. The subtopics are listed below.\n",
        "\n",
        "  Subopics list:\n",
        "  \"\"\" + subtopic + \"\"\"\n",
        "  Now please determine accurately the subtopic or subtopics based on these top words and general topic. You MUST ONLY choose from the subtopics list above. Choose to the best of your ability and do not leave the response as none. Please list out the topic and subtopics in this format: [topic - subtopic1|subtopic2|etc]. Some examples of the expected format are [physical data and processes - astrobiology|diffusion|gravitational lensing: strong], [astrometry and celestial mechanics - occultations], [stars - (stars:) circumstellar matter|stars: flare|stars: interiors|(stars:) supernovae: general]. Note these examples contain subtopics chosen ONLY from the subtopics list for each topic.\n",
        "  Then provide evidence by explaining which of the top words correspond with the subtopics. Here are the top words and the general topic:\n",
        "  \"\"\"\n",
        "  return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naOkE2Ngcuu9"
      },
      "outputs": [],
      "source": [
        "###IMPORTANT FUNCTIONS###\n",
        "\n",
        "def extract_topics(text, topic):\n",
        "  '''\n",
        "  Returns the topics/subtopics in a list format\n",
        "  '''\n",
        "  # Extract the list using a regular expression\n",
        "  match = re.search(r'\\[(.*?)\\]', text)\n",
        "  if match:\n",
        "    list_string = match.group(1)\n",
        "  else:\n",
        "    return 'None'\n",
        "  if topic == \"t\":\n",
        "      result_list = [item.strip().lower().replace('`', '') for item in list_string.split(',')]\n",
        "      return result_list\n",
        "  else:\n",
        "    return list_string\n",
        "\n",
        "\n",
        "def generate_subtopics(topwords, topic):\n",
        "  '''\n",
        "  Uses the llama model to generate the subtopics for a given topic\n",
        "  '''\n",
        "  try:\n",
        "    subtopics = d[topic]\n",
        "  except:\n",
        "    return \"[\"+topic + \" - not listed]\"\n",
        "\n",
        "  client = Groq()\n",
        "  completion = client.chat.completions.create(\n",
        "      model=\"llama3-70b-8192\",\n",
        "      messages=[\n",
        "          {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt_subtopic(subtopics) + \"Top Words: \" + str(topwords) + \" Topic: \" + topic\n",
        "          }\n",
        "        ],\n",
        "      temperature=0,\n",
        "      max_tokens=3000,\n",
        "      top_p=1,\n",
        "      stream=True,\n",
        "      stop=None,\n",
        "    )\n",
        "  response = \"\".join(chunk.choices[0].delta.content or \"\" for chunk in completion)\n",
        "  return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj-2ZrmesA5I"
      },
      "source": [
        "## Run the Llama Model\n",
        "In the following code, you will open a new csv file and name it. Then run the llama model and the csv file will update after each iteration. The model reads the topwords for a given researcher and returns the topics that match those words. Then it will run those topics and topwords through the model again but this time to gather the subtopics (gather_subtopics function is called)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocF2nr0lTKH1"
      },
      "outputs": [],
      "source": [
        "!pip install groq\n",
        "%env GROQ_API_KEY= #Insert you groqCloud API key here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LENxvwH6ULsw"
      },
      "outputs": [],
      "source": [
        "# @title Run this just once! If the code errors do not run this cell again!\n",
        "# Opens a new csv file with headers\n",
        "import csv\n",
        "\n",
        "output_file = path_stop+\"output_0-1000.csv\" #name the output file\n",
        "with open(output_file, 'a', newline='', encoding='utf-8') as dynamic_csv_file:\n",
        "  csv_writer = csv.writer(dynamic_csv_file)\n",
        "  csv_writer.writerow([\"Input Author\", \"Affiliations\", \"Combined Top Words\", \"Topics with Explanation\", \"Subtopics with Explanation\", \"Topics\", \"Subtopics\", \"Number of Papers\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhINtjUPFS4U"
      },
      "outputs": [],
      "source": [
        "from groq import Groq\n",
        "import re\n",
        "import csv\n",
        "\n",
        "%env GROQ_API_KEY= #Insert you groqCloud API key here\n",
        "client = Groq()\n",
        "num_iter = 0 #if the code errors change to the row number it stopped at next time you run this\n",
        "\n",
        "for topwords in datf[\"Top Combined Words\"]:\n",
        "  completion = client.chat.completions.create(\n",
        "      model=\"llama3-70b-8192\",\n",
        "      messages=[\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": prompt_topic + str(topwords)\n",
        "          }\n",
        "      ],\n",
        "      temperature=0,\n",
        "      max_tokens=3000,\n",
        "      top_p=1,\n",
        "      stream=True,\n",
        "      stop=None,\n",
        "  )\n",
        "\n",
        "  #Topics with explanation\n",
        "  response = \"\".join(chunk.choices[0].delta.content or \"\" for chunk in completion)\n",
        "  #Extract only the topic\n",
        "  topics = extract_topics(response, \"t\")\n",
        "  #Subtopics with explanation\n",
        "  full_st = []\n",
        "  #Extract only the subtopics\n",
        "  st = []\n",
        "\n",
        "  for topic in topics:\n",
        "    subtopics = generate_subtopics(topwords, topic)\n",
        "    full_st.append(subtopics)\n",
        "    st.append(extract_topics(subtopics, \"s\"))\n",
        "\n",
        "  #Get input author, affiliations, and number of papers\n",
        "  author = datf[\"Input Author\"][num_iter]\n",
        "  aff = datf[\"Affiliations\"][num_iter]\n",
        "  papers = datf[\"Number of Papers\"][num_iter]\n",
        "\n",
        "  #Write to csv\n",
        "  with open(output_file, 'a', newline='', encoding='utf-8') as dynamic_csv_file:\n",
        "    csv_writer = csv.writer(dynamic_csv_file)\n",
        "    csv_writer.writerow([author, aff, topwords, response, full_st, topics, st, papers])\n",
        "    num_iter += 1\n",
        "    print(\"Completed Row \" + str(num_iter))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CEBJldes2Bk"
      },
      "source": [
        "## Format and Save the Results\n",
        "Read in the output csv file. Then format the results in the following cell and export as an excel file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDIfAeuso-Ah"
      },
      "outputs": [],
      "source": [
        "output_file = path_stop+\"output_0-1000.csv\" #make sure the output file name is correct\n",
        "output_df = pd.read_csv(output_file)\n",
        "output_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im21mJegpuRB"
      },
      "outputs": [],
      "source": [
        "#Format the model results\n",
        "import ast\n",
        "def format_output(output):\n",
        "  lst = ast.literal_eval(output)\n",
        "  format = \"\"\n",
        "  for item in lst:\n",
        "    try:\n",
        "      topics = item.split(' - ')[0]\n",
        "      subtopic_split = item.split(' - ')[1]\n",
        "      subtopics = subtopic_split.split('|')\n",
        "      format += topics+':'+str(subtopics)+'|'\n",
        "    except:\n",
        "      pass\n",
        "  format+=\"Number of Papers: \"\n",
        "  return format\n",
        "\n",
        "output_df[\"Expertise\"] = output_df[\"Subtopics\"].apply(format_output)\n",
        "output_df[\"Expertise\"] = output_df['Expertise'] + output_df['Number of Papers'].astype(str)\n",
        "output_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4pao0nP8nou"
      },
      "outputs": [],
      "source": [
        "#Merge the SMS chunk of data with the output file\n",
        "final_df = sample_df.merge(output_df, left_on=\"Name\", right_on=\"Input Author\")\n",
        "final_df.rename(columns={\"Expertise_x\": \"Labeled Expertise\", \"Expertise_y\": \"Model Expertise\"}, inplace=True) #rename columns\n",
        "final_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HT6Y5G5eEwy_"
      },
      "outputs": [],
      "source": [
        "#Export the final merged dataframe to an excel file\n",
        "final_df.to_excel(path_stop+\"final_0-1000.xlsx\", index=False) #rename the file after each run"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}